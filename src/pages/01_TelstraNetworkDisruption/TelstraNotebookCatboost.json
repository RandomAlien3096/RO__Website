{"Telstra Network Disruption\n\n------------------------------------------------------------------------\n\nIng. Rafael Oliva\n\nGuatemala, 27 de Octubre de 2022\n\nEntendimiento de Negocio\n\n------------------------------------------------------------------------\n\nEl objetivo de este problema es poder predecir en la red de TELSTRA la\nseveridad de la falla en un momento dado en una ubicacion en particular\ncon una \"llave\" (id key). Estas predicciones son basadas en los datos\ndisponibles en nuestros archivos.\n\nEn esta situacion, se categorizo la severidad de las fallas en tres\ncategorias": null, "\n\n-   0 (No hay falla)\n-   1 (Pocas fallas)\n-   2 (Multiples fallas)\n\nAsi que, dado el problema proporsionado por TELSTRA, debemos de\nplantearnos las siguientes preguntas:\n\n-   La severidad de las fallas atravez de la red, estan directamente\n    relacionadas con la ubicacion?\n-   Se podra predecir la ubicacion de la falla dada la base de datos\n    proporcionada?\n-   Como podemos medir nuestro valor de exito dentro del proyecto?\n\nEntendimiento de Datos y Procesamiento de Datos\n\n------------------------------------------------------------------------\n\nEl conjunto de datos fue creado a partir de 6 reportes de tipos de\nfallas severidad de fallas y mas. Estos reportes representan el\nhistorial mas reciente de fallas y la ubicacion de las mismas alrededor\nde la red de comunicacion de la empresa Telstra. Para poder avanzar en\nla creacion de modelos de prediccion primero se debe determinar que\ncaracteristicas son las mas relevantes en todo este conjuto de\ninformacion proporcionada. Para ello realizaremos una analisis\nexploratorio de datos inicial. Esto es para tener un inicio para el\nplantamiento de la hipotesis inicial.\n\nAhora preprocesaremos los datos donde lograremos tener una mejor idea de\nla calidad de datos que se tiene y el tipo de datos con que se trabajara\n\n    %matplotlib inline\n    #Seccion de importacion de librerias utilizadas en el transcurso del proyecto\n    import numpy as np\n    import pandas as pd\n    import os\n    import seaborn as sns\n\n    import matplotlib.pyplot as plt\n    plt.rcParams['figure.dpi']": null, " es decir, si uno incrementa, el otro igual\nincrementa. Y donde -1 es que las variables son inversamente\nproporcionales.\n\nAhora bien, se denota a simple vista unas relaciones claves conforme a\nla ubicaion para nuestro analisis:\n\n-   Location v fault_severity\n-   location v severity_type\n\nAhora, despues de nuestra matriz de correlacion procederemos a\ninvestigar visualmente un poco mas sobre la relevancia que tiene la\nubicacion sobre el tipo de fallas que se encuentran. Para ello,\nutilizaremos una grafica de dispersion para visualizar la ubicacion de\ncada falla respecto a la informacion que se tiene.\n\n    #loading test.csv file\n    test_plot ": null, "\n\n[]\n\nYa vista la grafica de dispersion, podemos notar una tendencia. Entre\nmayor sea el numero de la ubicacion, las fallas se hacen mas propensas y\naumentan su severidad. Se puede asumir que, entre mayor sea el numero de\nla ubicacion, esta se encontra mas lejos. Y que por igual, que cada uno\nde las ubicaciones, dependiendo de su numero, se encuentran en orden y\nen cercania relativa al valor dado. A partir de ello, podemos plantear\nuna hipotesis con nuestra matriz de correlacion": null, " La severidad y cantidad\nde las fallas en la red de TELSTRA sera directamente proporcional a la\nubicacion y distancia que estas tengan.\n\nModelado\n\n------------------------------------------------------------------------\n\nDebido a la naturalidad categorica de nuestro complejo de datos, es\nnecesario, ya sea, procesar profundamente nuestro set de datos o\nutilizar un algoritmo de machine learning capaz de poder procesar esta\ninformacion. Dado esto, se propuso utilizar el algoritmo open-source\nllamado \"CatBoost\". El cual es un algoritmo bastante versatil y\nflexible. Donde tiene la capacidad de manejar una gran variedad de tipo\nde datos sin ningun problema proveyendo soluciones fuera de lo\nconvencional para apoyar con los problemas dados comunmente en el\nanalisis de negocios y big data.\n\nEl objetivo ahora sera desplegar el algoritmo CatBoost de forma\ncorrecta. Para ello, nececitamos iniciar con el entrenamiento\nsupervisado del complejo de datos ya procesado. Con ello utilizaremos el\ncomplejo de datos \"train\" donde utilizaremos el 75% de sus datos para\nentrenar al algoritmo y el 25% seran utilizados para validar el\nentrenamiento y simular a su vez como este se comportara si es dado un\nset de datos no antes vistos.\n\n    #Splitting X(data in array) and y (index of data used)\n\n    X ": null, "\n\n[]\n\nDespliegue\n\n------------------------------------------------------------------------\n\nEn esta fase, evaluaremos el peso, o mas bien, la importancia de cada\nuna de las caracteristicas utilizadas para el entrenamiento de nuestro\nmodelo. Ya que, esto nos permitira ver que mas se requiere para poder\nmejorar el modelo\n\n    model.get_feature_importance()\n\n    array([ 6.3250055 , 22.46957826, 11.11821714, 12.5966741 , 16.14897793,\n           21.25928377, 10.08226331])\n\nEsta funcion nos da a conocer el porcentaje de relevancia de cada una de\nlas caracteristicas utilizadas. Donde:\n\n-   id ": null, "\n    volumen y en especifico, log_feature. Log_feature, y volume, existen\n    en el mismo archivo, lo cual es un indicativo que van desde el\n    inicio, relacionados. Por igual, el detalle de log_feature nos logra\n    hacer entender que debe de exisitir algo mas haya que no logramos\n    ver a simple vista en este modelo. Algo que tenga que ver con el\n    historial del tiempo de cada uno de estos errores. Saber esto, nos\n    podra permitir conocer como mejorar el modelo y por ende, tenga un\n    entrenamiento mas duradero y de resultados mas precisos.": null}